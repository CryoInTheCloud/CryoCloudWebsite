{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927cf40c-fe04-439b-a065-07daa155b522",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Using ICESat-2 ATL15 (Gridded Arctic Land Ice Height) to investigate ice-surface height anomalies\n",
    "\n",
    "### Written by \n",
    "- Wilson Sauthoff (https://wsauthoff.github.io)\n",
    "- Luis Lopez (https://betolink.dev/)\n",
    "- Jessica Scheick (https://eos.unh.edu/person/jessica-scheick)\n",
    "- Tasha Snow (https://tsnow03.github.io)\n",
    "\n",
    "### Key learning outcomes:\n",
    "- How to gather data from disparate sources.\n",
    "- What is a Coordinate Reference System (CRS) and why it matters.\n",
    "- How to use geometries including Points and Polygons to define an area of interest and subset data. \n",
    "- The basics of how the icepyx library simplifies obtaining and interacting with ICESat-2 data. \n",
    "- How Xarray can simplify the import of multi-dimensional data.\n",
    "- Open, plot, and explore gridded raster data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3fc55-9a2e-458e-a0a9-3354be7420a5",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Computing environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452d675-7c0a-4081-8e85-300a53997e2c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We will set up our computing environment with library imports and utility functions\n",
    "\n",
    "Tip: If you need to import a library that is not pre-installed, use `%pip install <package-name>` alone within a Jupyter notebook cell to install it for this instance of CryoCloud (the pip installation will not presist between logins. All of the libraries we intend to use are pre-installed so we can skip this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e226454-c730-4656-ae88-19297a8a855a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Needed for pandas read_excel\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2db536-38fa-40b2-b01e-6f2f97881789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# import internal libraries\n",
    "import os\n",
    "\n",
    "# import external libraries\n",
    "import earthaccess\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "import hvplot.xarray\n",
    "import icepyx as ipx\n",
    "# import matplotlib as mpl\n",
    "# mpl.use('TkAgg') \n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Transformer\n",
    "import rioxarray\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "# define utility function\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Greenland (epsg:3413) coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    Inputs\n",
    "    * lon, lat in decimal degrees (lon: W is negative; lat: S is negative)\n",
    "    Outputs\n",
    "    * x, y in [m]\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3413\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ded922-7e2e-42a4-9435-5399d92fb289",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Greenland subglacial lakes\n",
    "\n",
    "There are two classes of subglacial lakes: stable and active. Stable lakes have had a stable volume over the observational record whereas active episodically drain and fill. We can observe active subglacial lakes using ice-surface deformation time series. \n",
    "\n",
    "In this tutorial, we will focus on Greenland where active subglacial lakes have been inferred from surface height changes. \n",
    "\n",
    "Northern hemisphere subglacial lakes compiled in a global inventory by Livingstone and others (2020):\n",
    "\n",
    "<img src=\"images/Livingstone_2020_Fig3a.png\" alt=\"2022 inventory of subglacial lakes\" class=\"bg-primary mb-1\" width=\"600px\">\n",
    "\n",
    "We can investigate active subglacial lakes using ice surface height anomalies as the overlying ice deforms when active lakes drain and fill episodically. [These videos](https://svs.gsfc.nasa.gov/4913) from NASA's Science Visualization Studio illustrate how we observe active lakes filling and draining through time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488386a-e802-49bc-b93e-bd5d031eec55",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Locating the Greenland subglacial lakes\n",
    "Consulting the supplementary data of the [Livingstone and others (2020) inventory](https://www.nature.com/articles/s43017-021-00246-9#Sec16), we can construct a [geopandas geodataframe](https://geopandas.org/en/stable/gallery/create_geopandas_from_pandas.html) to investigate Greenland's subglacial lakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd201d-2c1a-492f-ab82-4582787f8236",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Opening non-cloud-hosted data uploaded to your CryoCloud Jupyter hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36750e1-eb53-426b-bc86-d09d9cf0c3cb",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If we are working with a dataset that is not cloud hosted or unavailable via a URL, we can upload that dataset to CryoCloud. \n",
    "\n",
    "First, decide where the uploaded data will live on your CryoCloud hub. It could be a data directory folder that you create in your CryoCloud's base directory or put the data into your working directory (whichever file management technique you prefer). \n",
    "\n",
    "Second, download the paper's supplementary data by [clicking here](https://static-content.springer.com/esm/art%3A10.1038%2Fs43017-021-00246-9/MediaObjects/43017_2021_246_MOESM1_ESM.xlsx). \n",
    "\n",
    "Third, upload the supplementary data spreadsheet into your new data directory folder or your working directory. The supplementary data spreadsheet (.xlsx) is already uploaded to our working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d4ad8-cef2-4d6e-bc88-2ee1d3673c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in spreadsheet using pandas read_excel\n",
    "use_cols = ['Name / Location', 'Lat. oN', 'Lon. oE', 'Lake Type', 'References']\n",
    "import_rows = np.arange(0,65)\n",
    "# You may add a path/to/file if you'd like to try reading spreadsheet from where you saved it in a data directory \n",
    "df = pd.read_excel('43017_2021_246_MOESM1_ESM.xlsx', 'Greenland', usecols=use_cols, skiprows = lambda x: x not in import_rows)\n",
    "\n",
    "# View pandas dataset head (first 5 rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b979f8c-8bb5-446c-8511-283025ff945d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "However, since this data has a direct download URL, so we can read the data directly into CryoCloud skipping the download and upload steps, like so: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff870cc-3b5e-42df-8ce0-a3c256ad4ffd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Open data directly via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047bbc78-a049-4055-8002-425d6881b70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in spreadsheet using pandas read_excel\n",
    "url = 'https://static-content.springer.com/esm/art%3A10.1038%2Fs43017-021-00246-9/MediaObjects/43017_2021_246_MOESM1_ESM.xlsx'\n",
    "use_cols = ['Name / Location', 'Lat. oN', 'Lon. oE', 'Lake Type', 'References']\n",
    "import_rows = np.arange(0,65)\n",
    "df = pd.read_excel(url, sheet_name='Greenland', usecols=use_cols, skiprows = lambda x: x not in import_rows)\n",
    "\n",
    "# View pandas dataset head (first 5 rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d0285-380e-4e16-9979-73f73170e1cd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This is looking good, but we can make it even better by storing the data in a GeoPandas GeoDataFrame which offers additional functionality beyond pandas. You can add a geometry column of Shapely objects that make geospatial data processing and visualization easier. Here we have Shapely points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7195c0-606b-4467-8b2d-1deb94030b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create GeoPandas GeoDataFrame from Pandas DataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df['Lon. oE'], df['Lat. oN']))\n",
    "\n",
    "# Set the Coordinate Reference System (CRS) of the geodataframe\n",
    "if gdf.crs is None: \n",
    "    # set CRS WGS84 in lon, lat\n",
    "    gdf.set_crs('epsg:4326', inplace=True)\n",
    "    \n",
    "# Display GeoDataFrame\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c3f03-ffe6-44fa-9128-b83d7d88af33",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's look at Greenland's active subglacial lake inventory by filtering on the Lake Type column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1751a6-f1fa-4e66-9bc4-c0fed6dcf6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at Greenland's active subglacial lake inventory\n",
    "gdf[gdf['Lake Type'] == 'Active']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b6465-dfff-4c1a-bf4c-44adee10cda1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now let's plot the lake locations to ensure our data read-in went as expected. But first, we need to make sure all projections for the data we want to show are the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc8db1-be7f-4933-8b1f-64cbc6af90bc",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## What is a CRS/EPSG?\n",
    "\n",
    "- A Coordinate Reference System (CRS) tells you how the Earth's 3-D surface is projected onto a 2-D plane map. Below are examples of various projections of continguous USA from an excellent [Data Carpentry tutorial on the CRS topic](https://datacarpentry.org/organization-geospatial/03-crs/): \n",
    "\n",
    "<img src=\"images/DataCarpentryCRS.jpg\" alt=\"Coordinate Reference System comparison\" class=\"bg-primary mb-1\" width=\"600px\">\n",
    "\n",
    "- A particular CRS can be referenced by its EPSG code (e.g., epsg:4326 as we used in the GeoPandas GeoDataFrame example above). The EPSG is a structured dataset of CRS and Coordinate Transformations. It was originally compiled by the now defunct European Petroleum Survey Group (EPSG) but continues to be one common way of representing a specific CRS.\n",
    "- Most map projections make land areas look proportionally larger or smaller than they actually are. Below is intuitive visualization of this from another excellent site of tutorials on geospatial topics called [Earth Lab](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-to-coordinate-reference-systems/): \n",
    "\n",
    "<img src=\"images/EarthLabProjectionDistortion.jpg\" alt=\"How projections distort a human head\" class=\"bg-primary mb-1\" width=\"600px\">\n",
    "\n",
    "- Because of this distortion, we select a projection that is best suited for the geographic region we are studying to minimize distortions.\n",
    "- Previously we used epsg:4326, which is a geographic coordinate system that makes use of the World Geodetic System 1984 (WGS84) ellipsoid as a datum reference elevation. A datum consists in an ellipsoid relative to which the latitude and longitude of points are defined, with a geoid defining the surface at zero height (visual from [ESA's navipedia](https://gssc.esa.int/navipedia/index.php/Regional_Datums_and_Map_Projections)):\n",
    "\n",
    "<img src=\"images/ESANavipediaDatum.png\" alt=\"What is a datum\" class=\"bg-primary mb-1\" width=\"600px\">\n",
    "\n",
    "- A geographic coordinate system is more than just the ellipsoid. It adds a coordinate system and units of measurement. epsg:4326 uses the very familiar longitude and latitude and degrees as the coordinate system and units. We selected epsg:4326 because longitude and latitude were the coordinates in the dataset.\n",
    "- There are numerous formats that are used to document a CRS. Three common formats include: proj.4, EPSG, and Well-known Text (WKT) formats.\n",
    "- Useful websites to look up CRS strings are [spatialreference.org](Spatialreference.org) and [espg.io](https://epsg.io/). You can use the search on these sites to find an EPSG code.\n",
    "- Often you have data in one format or projection (CRS) and you need to transform it to a more regionally accurate CRS or match it to another datasets' CRS to plot them together. Let’s transform our geopandas data to the NSIDC Sea Ice North Polar Stereographic projection (epsg:3413) easily visualize our lakes on a projection that minimizes distortion of our area of interest (Greenland). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b2914-2a57-40aa-ba26-113e15adf373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load \"Natural Earth” countries dataset, bundled with GeoPandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "Greenland = world[world['name'] == 'Greenland']\n",
    "\n",
    "# Create a rough plot to ensure data was read properly\n",
    "fig, axs = plt.subplots(1,2, figsize=(7,6.5))\n",
    "Greenland.plot(ax=axs[0], facecolor='lightgray', edgecolor='gray')\n",
    "gdf[gdf['Lake Type'] == 'Active'].plot(ax=axs[0], label='active subglacial lake')\n",
    "axs[0].set_xlabel('northing (m)'); axs[0].set_ylabel('easting (m)')\n",
    "axs[0].set_title('epsg:4326: WGS84\\nWorld Geodetic System 1984')\n",
    "axs[0].legend()\n",
    "             \n",
    "Greenland.to_crs('epsg:3413').plot(ax=axs[1], facecolor='lightgray', edgecolor='gray')\n",
    "gdf[gdf['Lake Type'] == 'Active'].to_crs('epsg:3413').plot(ax=axs[1], label='active subglacial lake')\n",
    "axs[1].set_xlabel('northing (m)'); axs[1].set_ylabel('easting (m)')\n",
    "axs[1].set_title('epsg:3413: WGS84 / NSIDC \\nSea Ice Polar Stereographic North')\n",
    "axs[1].legend()\n",
    "fig.suptitle('Active subglacial lake distribution\\nacross Greenland in two projections')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bccee8-6a13-40dd-b59e-ad96602b54d7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Using the Shapely Polygon geometry to create a search radius and subset data\n",
    "- The Shapely points used before are useful to tell us the centroid of the lakes.\n",
    "- However, we'd like to be able to create a search radius around that point to search for and subset data. \n",
    "- Some datasets are massive, so using Shapely polygons as a search radius around your region of interest will speed up your computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e94b90-a04d-4d8a-9a8b-fb499f9bbc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create GeoSeries of our GeoDataFrame that converts to Arcic polar stereographic projection \n",
    "# and makes 10-km radius buffered polygon around each lake point\n",
    "gs = gdf.to_crs('3413').buffer(10000)\n",
    "\n",
    "# Create a copy of the original GeoPandas GeoDataFrame so that we don't alter the original\n",
    "gdf_polys = gdf.copy(deep=True)\n",
    "\n",
    "# Create new GeoDataFrame to store the polygons\n",
    "gdf_polys = gpd.GeoDataFrame(gdf, \n",
    "                             # We use the GeoDataFrame (gdf) copy, gdf_polys, and replaced its geometry with the GeoSeries of polygons\n",
    "                             geometry=gs, \n",
    "                             # We must specify the crs, so we use the one used to create the GeoSeries of polygons\n",
    "                             crs='epsg:3413'\n",
    "                             # But then we switch the crs back to good ol' 4623 lon, lat\n",
    "                             ).to_crs('4623')\n",
    "\n",
    "# Look at active lakes to ensure it worked as expected\n",
    "gdf_polys[gdf_polys['Lake Type'] == 'Active']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac86c8-710f-4c6b-926d-7cac63c69572",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now that we know our active lake locations, let’s grab data to study the filling and draining of the lakes. We will use ICESat-2 surface elevations to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96604dff-c7d7-4117-b859-326545f486bc",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## What is ICESat-2?\n",
    "\n",
    "ICESat-2 (Ice, Cloud, and land Elevation Satellite 2), part of NASA's Earth Observing System, is a satellite mission for measuring ice sheet elevation and sea ice thickness, as well as land topography, vegetation characteristics, and clouds. It does so using an altimeter or an altitude meter, which is an instrument used to measure the altitude of an object above a fixed level (the datum we talked about earlier). This is typically achieved by measuring the time it takes for a lidar or radar pulse, released by a satellite-based altimeter, to travel to a surface, reflect, and return to be recorded by an onboard instrument. ICESat-2 uses three pairs of laser pulsers and the detector to count the reflected photons. \n",
    "\n",
    "ICESat-2 laser configuration (from [Smith and others, 2019](https://doi.org/10.1016/j.rse.2019.111352)): \n",
    "\n",
    "<img src=\"images/Smith_2019_fig1.jpg\" alt=\"ICESat-2 laser configuration\" class=\"bg-primary mb-1\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c610d-4406-49a5-85be-b26c98a32af0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## What is ATL14/15?\n",
    "ATL15 is one of the various [ICESat-2 data products](https://icesat-2.gsfc.nasa.gov/science/data-products). ATL15 provides various resolutions (1 km, 10 km, 20 km, and 40 km) of gridded raster data of height change at 3-month intervals, allowing for visualization of height-change patterns and calculation of integrated active subglacial lake volume change (Smith and others, 2022).\n",
    "\n",
    "ATL14 is an accompnaying high-resolution (100 m) digital elevation model (DEM) that provides spatially continuous gridded data of ice sheet surface height.\n",
    "\n",
    "Learn more about the ICESat-2 ATL14/15 Gridded Antarctic and Arctic Land Ice Height Change data product dataset [here](https://doi.org/10.5067/ATLAS/ATL15.002)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2863c3-3967-47a4-8be6-2eee3d6af0ed",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Streaming cloud-hosted data from NASA Earth Data Cloud\n",
    "We will be working with cloud-hosted data files. This [guide](https://nsidc.org/data/user-resources/help-center/nasa-earthdata-cloud-data-access-guide) explains how to find and access Earthdata cloud-hosted data. [Here](https://nsidc.org/data/earthdata-cloud) is a complete list of earthdata cloud-hosted data products currently available from NSIDC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0209d-f39a-4566-8966-6798631be98d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Using icepyx to simplify searching for ICESat-2 data\n",
    "[icepyx](https://icepyx.readthedocs.io/en/latest/) is a community and Python software library that simplifies the process of searching (querying), accessing (via download or in the cloud), and working with (including subsetting, visualizing, and reading in) ICESat-2 data products. A series of [examples](https://github.com/icesat2py/icepyx/tree/main/doc/source/example_notebooks) introduce users to its functionality, and the icepyx community always welcomes new members, feature requests, bug reports, etc.\n",
    "\n",
    "To search for data spatially, icepyx accepts shapefiles, kml files, and geopackage files, as well as bounding boxes and polygons, as input bounding regions. We have two options for supplying this information: (1) save one of the lakes' geometries as a geopackage or (2) extract the exterior coordinates and supply them directly. You may run either of the next two cells; then we'll use the spatial information to query ICESat-2 data for that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea8f8c-ce47-4090-919f-d30863309495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1) save one of the lakes' geometries as a geopackage\n",
    "# Export to geopackage to subset ICESat-2 data\n",
    "gdf_polys[gdf_polys['Name / Location'] == 'Flade Isblink ice cap'].to_file(os.getcwd() + '/Flade_Isblink_poly.gpkg')\n",
    "spatial_extent = 'Flade_Isblink_poly.gpkg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f1628-0fc1-407c-a8e5-a85cb8735345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2) use the shapely package to supply the polygon coordinates directly as a list\n",
    "from shapely.geometry import mapping\n",
    "spatial_extent = list(mapping(gdf_polys[gdf_polys['Name / Location'] == 'Flade Isblink ice cap'].geometry.iloc[0])[\"coordinates\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15602f-3fc1-40de-9519-a75ae5b3b4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specifying the necessary icepyx parameters\n",
    "short_name = 'ATL15'  # The data product we would like to query\n",
    "date_range = ['2018-09-15','2023-03-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a352b60-b203-47d6-9785-2cfdbcfdeb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the Query object\n",
    "region = ipx.Query(short_name, spatial_extent, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f484f-f476-4547-ac9b-6637fe880758",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "We can visualize our spatial extent on an interactive map with background imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852347c-fafd-4db0-b81e-e70e3e98ed98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize area of interest\n",
    "region.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bdc52-4de8-44fa-8f8e-fcdd89da01bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's find out some information about the available data granuales (files)\n",
    "region.avail_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc30f0-dc2e-4dc4-a13b-305944246691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's see the granule IDs and cloud access urls\n",
    "gran_ids = region.avail_granules(ids=True, cloud=True)\n",
    "gran_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e741b-9f35-4598-accf-827e4eb6b447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's grab the s3 URL of the highest resolution available product\n",
    "s3url = gran_ids[1][3]\n",
    "s3url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd249ae-dbe8-49c9-90e4-b291613659bf",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "You can manually find s3 URL's for cloud-hosted data from [NASA Earth Data](https://www.earthdata.nasa.gov/)\n",
    "\n",
    "Learn more about finding cloud-hosted data from NASA Earth data cloud [here](https://nsidc.org/data/user-resources/help-center/nasa-earthdata-cloud-data-access-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba938e7-0ac0-4c6f-99f1-854a57ac2779",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The next step (accessing data in the cloud) requires a NASA Earthdata user account.\n",
    "You can register for a free account [here](https://www.earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/earthdata-login).\n",
    "We provide two options for reading in your data: (1) by setting up an s3 file system and using Xarray directly; or (2) by using icepyx (which uses Xarray under the hood).\n",
    "Currently, the read time is similar with both methods.\n",
    "The h5coro library will soon be available to help speed up this process.\n",
    "\n",
    "The file system method requires you complete a login step.\n",
    "icepyx will automatically ask for your credentials when you perform a task that needs them.\n",
    "If you do not have them stored as environment variables or in a .netrc file, you will be prompted to enter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af9f40-c9a8-48ed-9f81-c93d52d960e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1) authenticate\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e81f6-9092-4523-8565-34a75d8631f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1) set up our s3 file system using our credentials\n",
    "s3 = earthaccess.get_s3fs_session(daac='NSIDC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377ae88-8552-4ee8-8595-d15b786b003c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (1) Open s3url data file and store in Xarray Dataset\n",
    "# This cell takes 10s of secs to load\n",
    "with s3.open(s3url,'rb') as f:\n",
    "    # ATL15_dh = rioxarray.open_rasterio(f, group='delta_h').load()  # FIXME: preferred, but giving error\n",
    "    ATL15_dh = xr.open_dataset(f, group='delta_h').load()\n",
    "\n",
    "# View Xarray Dataset\n",
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e72f9a-c6bc-448c-891a-3b4870c0ecea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2) create a Read object; you'll be asked to authenticate at this step if you haven't already\n",
    "reader = ipx.Read(s3url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1ab31-ac90-4031-8a78-39bf26828c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2) if you are running not running icepyx from the development branch, you must\n",
    "# manually set the version while initializing the variables sub-object\n",
    "# otherwise reader.vars.append() will fail because of a bad version value in the ATL15 metadata\n",
    "# We're working with NSIDC to resolve this\n",
    "# reader._read_vars = ipx.core.variables.Variables(\n",
    "#     product=reader.product, \n",
    "#     version=ipx.core.is2ref.latest_version(reader.product)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc752711-13aa-4d0a-b15d-b21a79894b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2) see what variables are available\n",
    "reader.vars.avail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e93f16-b98e-4690-b273-aeeadb5bfdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2) Indicate which variables you'd like to read in.\n",
    "# More information on managing ICESat-2 variables is available in the icepyx documentation and examples.\n",
    "reader.vars.append(keyword_list=[\"delta_h\"])\n",
    "# view the variables that will be loaded into memory in your DataSet\n",
    "reader.vars.wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475a92f-4b55-4307-acca-1c16f76144c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (2) load your data into memory\n",
    "# if you are asked if you want to proceed, enter 'y' and press return/enter\n",
    "# Depending on your hub settings, the warning letting you know this operation will take a moment may or may not show up\n",
    "ATL15_dh = reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589974d-78ec-4747-a4de-223d17252dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e5f8a-a7fb-4f76-b70d-cd9c55c94f08",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can acquaint ourselves with this dataset in a few ways: \n",
    "- The data product's [overview page](https://doi.org/10.5067/ATLAS/ATL15.002) (Smith and others, 2022) to get the very basics such as geographic coverage, CRS, and what the data product tells us (quarterly height changes).\n",
    "- The Xarray Dataset read-in metadata: clicking on the written document icon of each data variable will expand metadata including a data variable's dimensions, datatype, etc. \n",
    "- The data product's [data dictionary](https://nsidc.org/sites/default/files/documents/technical-reference/icesat2_atl15_data_dict_v002.pdf) (Smith and others, 2021) to do a deep dive on what individual variables tell us. \n",
    "- THe data product's [Algorithm Theoretical Basis Document](https://icesat-2.gsfc.nasa.gov/sites/default/files/page_files/ICESat2_ATL14_ATL15_ATBD_r001.pdf)\n",
    "\n",
    "We'll be plotting the delta_h data variable in this tutorial, here's what we can learn about from these sources:\n",
    "- [ATL14/15's overview page](https://doi.org/10.5067/ATLAS/ATL15.002): this is likely the 'quarterly height changes' described, but let's dive deeper to be sure\n",
    "- ATL14/15's Xarray Dataset imbedded metadata tells us a couple things: delta_h =height change  at 1 km (the resolution selected earlier) and height change relative to the datum (Jan 1, 2020) surface\n",
    "- [ATL14/15's data dictionary](https://nsidc.org/sites/default/files/documents/technical-reference/icesat2_atl15_data_dict_v002.pdf): delta_h = quarterly height change at 40 km\n",
    "\n",
    "Ok, since the data is relative to a datum, we have two options: \n",
    "1) Difference individual time slices to subtract out the datum, like so: \n",
    "\n",
    "    (time$_0$ - datum) - (time$_1$ - datum) = time$_0$ - datum - time$_1$ + datum = time$_0$ - time$_1$\n",
    "\n",
    "2) Subtract out the datum directly. The datum is the complementary dataset high-resolution DEM surface contained in tha accompanying dataset ATL14.\n",
    "\n",
    "In this tutorial we'll use the first method. We'll use some explanatory data analysis to illustrate this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9c230-1b92-4468-8231-5f90a9f119ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make a simple plot of the first minus the zeroth time slices\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "dhdt = ATL15_dh['delta_h'][1,:,:] - ATL15_dh['delta_h'][0,:,:]\n",
    "cb = ax.imshow(dhdt, origin='lower', norm=colors.CenteredNorm(), cmap='coolwarm_r', \n",
    "               extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "ax.set_xlabel('longitude'); ax.set_ylabel('latitude')\n",
    "plt.colorbar(cb, fraction=0.02, label='height change [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c743f6-7df7-4bd4-a407-547179f985d3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Hmmm...doesn't look like much change over this quarter. Why? Check out the bounds of the colorbar, we've got some pretty extreme values (colorbar is defaulting to ±10 m!) that appear to be along the margin. It's making more sense now. We can change the bounds of the colorbar to plot see more of the smaller scale change in the continental interior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b2eae-00fc-4a4b-8343-28b38688433a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's calculate some basic stats to determine appropriate coloarbar bounds\n",
    "print(dhdt.min().values)\n",
    "print(dhdt.max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb944c-b268-44cb-a4f3-690e8a7ae8ef",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can use a TwoSlopeNorm to achieve different mapping for positive and negative values while still keeping the center at zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7223f-02fa-4570-9704-9c631b3b996b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can make that same plot with more representative colorbar bounds\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "dhdt = ATL15_dh['delta_h'][1,:,:] - ATL15_dh['delta_h'][0,:,:]\n",
    "divnorm = colors.TwoSlopeNorm(vmin=dhdt.min(), vcenter=0, vmax=dhdt.max())\n",
    "cb = ax.imshow(dhdt, origin='lower', norm=divnorm, cmap='coolwarm_r', \n",
    "               extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "ax.set_xlabel('longitude'); ax.set_ylabel('latitude')\n",
    "plt.colorbar(cb, fraction=0.02, label='height change [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b76be-6df8-4cb1-b475-d962a7d7eb06",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now the colorbar bounds are more representative, but we still have the issue that extreme values along the margin are swapping any signals we might see in the continental interior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82678fa1-fd9a-469f-9eac-2254624f5e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's use the Xarray DataArray quantile method to find the 1% and 99% quantiles (Q1 and Q3) of the data\n",
    "dhdt.quantile([0.01,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff22ef-89ee-4f85-9b14-e46dfe528978",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can use these quantiles as the colorbar bounds so that we see the data variability by plotting the most extreme values at the maxed out value of the colorbar. We'll adjust the caps of the colorbar (using `extend`) to express that there are data values beyond the bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320be7d-73dd-45ec-8a7f-7efdb9b629d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make the same plot but using the quantiles as the colorbar bounds\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "dhdt = ATL15_dh['delta_h'][1,:,:] - ATL15_dh['delta_h'][0,:,:]\n",
    "divnorm = colors.TwoSlopeNorm(vmin=dhdt.quantile(0.01), vcenter=0, vmax=dhdt.quantile(0.99))\n",
    "cb = ax.imshow(dhdt, origin='lower', norm=divnorm, cmap='coolwarm_r', \n",
    "               extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "ax.set_xlabel('longitude'); ax.set_ylabel('latitude')\n",
    "plt.colorbar(cb, fraction=0.02, extend='both', label='height change [m]')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dab15-820d-45d9-bf17-72a8702af057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make the same plot but for all the available time slices and let's turn it in a function so that we can reuse this code for a smaller subset of data\n",
    "# create empty lists to store data\n",
    "def plot_icesat2_atl15(xmin, xmax, ymin, ymax, dataset):\n",
    "    # subset data using bounding box in epsg:3134 x,y\n",
    "    mask_x = (dataset.x >= xmin) & (dataset.x <= xmax)\n",
    "    mask_y = (dataset.y >= ymin) & (dataset.y <= ymax)\n",
    "    ds_sub = dataset.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Create empty lists to store data\n",
    "    vmins_maxs = []\n",
    "\n",
    "    # Find the min's and max's of each inter time slice comparison and store into lists\n",
    "    for idx in range(len(ds_sub['time'].values)-1): \n",
    "        dhdt = ds_sub['delta_h'][idx+1,:,:] - ds_sub['delta_h'][idx,:,:]\n",
    "        vmin=dhdt.quantile(0.01)\n",
    "        vmins_maxs += [vmin]\n",
    "        vmax=dhdt.quantile(0.99)\n",
    "        vmins_maxs += [vmax]\n",
    "        if (min(vmins_maxs)<0) & (max(vmins_maxs)>0):\n",
    "            vcenter = 0\n",
    "        else: \n",
    "            vcenter = max(vmins_maxs) - min(vmins_maxs)\n",
    "        divnorm = colors.TwoSlopeNorm(vmin=min(vmins_maxs), vcenter=vcenter, vmax=max(vmins_maxs))\n",
    "\n",
    "    # create fig, ax\n",
    "    fig, axs = plt.subplots(7,2, sharex=True, sharey=True, figsize=(10,10))\n",
    "\n",
    "    idx = 0\n",
    "    for ax in axs.ravel():   \n",
    "        ax.set_aspect('equal')\n",
    "        dhdt = ds_sub['delta_h'][idx+1,:,:] - ds_sub['delta_h'][idx,:,:]\n",
    "        cb = ax.imshow(dhdt, origin='lower', norm=divnorm, cmap='coolwarm_r', \n",
    "                       extent=[xmin[0], xmax[0], ymin[0], ymax[0]])\n",
    "        # Change polar stereographic m to km\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax.xaxis.set_major_formatter(ticks_x)\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax.yaxis.set_major_formatter(ticks_y)\n",
    "        # Create common axes labels\n",
    "        fig.supxlabel('easting (km)'); fig.supylabel('northing (km)')\n",
    "        # Increment the idx\n",
    "        idx = idx + 1\n",
    "     \n",
    "    fig.colorbar(cb, extend='both', ax=axs.ravel().tolist(), label='height change [m]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c6fa1-5013-46d8-80a9-1ca545e17611",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's zoom into an individual active lake to see more detail. First let's remind ourselves of the Greenland active subglacial lakes by filtering on lake type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6cefc-7794-45e3-a5d6-3900d7204d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf_polys_active = gdf_polys[gdf_polys['Lake Type'] == 'Active']\n",
    "gdf_polys_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54bca1-def0-46cd-b2c0-d08a1e79611d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can call the bounds of the geometry of the Shapely Polygon we created earlier\n",
    "gdf_sub = gdf_polys_active[gdf_polys_active['Name / Location'] == 'Inuppaat Quuat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb4599-4193-4c0e-8a9e-e96a01fcdb01",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now using these geometry bounds we can plot the area immediately around the active subglacial lake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e949e39-f84f-42ff-8e7f-7b45270d9a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the figure doesn't pop up, it is because the holoviews plot was run immediately after and interferred with \n",
    "# matplotlib. Just rerun this cell and it will be fixed\n",
    "%matplotlib widget\n",
    "\n",
    "# Assigning the min, max lon, lat coords of the lower-left and upper-right corners of our bounding box \n",
    "# around the lake's buffer polygon\n",
    "lon_min = gdf_sub.geometry.bounds.minx\n",
    "lon_max = gdf_sub.geometry.bounds.maxx\n",
    "lat_min = gdf_sub.geometry.bounds.miny\n",
    "lat_max = gdf_sub.geometry.bounds.maxy\n",
    "\n",
    "# Now we re-project the lon, lat into x, y coords in the 3413 CRS, which ATL15 uses\n",
    "xmin, ymin = ll2ps(lon_min, lat_min)\n",
    "xmax, ymax = ll2ps(lon_max, lat_max)\n",
    "\n",
    "# Use our plotting fuction to plot up the data based on min, max x, y bounds\n",
    "plot_icesat2_atl15(xmin, xmax, ymin, ymax, ATL15_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a59c54-792e-4a6b-8cbb-590d555405a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's explore plotting the data interactively using Xarray and Holoviews\n",
    "hvplot.extension('matplotlib')\n",
    "divnorm = colors.TwoSlopeNorm(vmin=-0.25, vcenter=0, vmax=1.25)\n",
    "ATL15_dh['delta_h'].hvplot(groupby='time', cmap='coolwarm_r', norm=divnorm, invert=True, \n",
    "                           width=(ATL15_dh['x'].max()-ATL15_dh['x'].min())/3e3, height=(ATL15_dh['y'].max()-ATL15_dh['y'].min())/3e3, \n",
    "                           widget_type='scrubber', widget_location='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4a56e-23dd-492d-b433-14412e22590a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up environment by deleting intermediary files\n",
    "os.remove('Flade_Isblink_poly.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373dba30-f76d-4eea-8a04-8105531fab0a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Streaming cloud-hosted data via earthaccess\n",
    "We will next use earthaccess to authenicate for NASA EarthData, and search and stream cloud-hosted data directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a0c7c-cb2f-4594-90f0-1210e0c8f812",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "<!-- DELETE -->\n",
    "<!-- ***\n",
    "IMPORTANT: Restart the kernel to ensure these recent changes are implemented in the computing environment.\n",
    "On CryoCloud's top menu: click Kernel>Restart Kernel...\n",
    "*** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bcb88-0bcc-40bc-8a83-fdef598b0c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import earthaccess Library and view version\n",
    "import earthaccess\n",
    "print(f\"Using earthaccess v{earthaccess.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669aa7c-325b-4ce3-850f-01d272c73b01",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Log into NASA's Earthdata using the earthaccess package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c436c5-9a3c-487c-b196-a67e80e01b3f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "There are multiple ways to provide your Earthdata credentials via [earthaccess](https://nsidc.github.io/earthaccess/). The [earthaccess authentication class](https://nsidc.github.io/earthaccess/tutorials/restricted-datasets/#auth) automatically tries three methods for getting user credentials to log in:\n",
    "1) with `EARTHDATA_USERNAME` and `EARTHDATA_PASSWORD` environment variables\n",
    "2) through an interactive, in-notebook login (used below); passwords are not shown plain text\n",
    "3) with stored credentials in a .netrc file (not recommended for security reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be86745-179a-4010-b033-3e2e10438fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to authenticate\n",
    "auth = earthaccess.login()\n",
    "print(auth.authenticated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51385d73-f828-48ba-9b69-a93473adfea0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Search for cloud-available datasets from NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e74d67-7dc5-4404-a8fe-fbec06801bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using a keyword search\n",
    "\n",
    "from pprint import pprint\n",
    "datasets = earthaccess.search_datasets(keyword=\"SENTINEL\",\n",
    "                                       cloud_hosted=True)\n",
    "\n",
    "for dataset in datasets[0:2]:\n",
    "    pprint(dataset.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a6bb3-9ca7-4353-99e8-b3acb0691f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using a known short name\n",
    "datasets = earthaccess.search_datasets(short_name=\"HLSS30\",\n",
    "                                       cloud_hosted=True)\n",
    "for dataset in datasets:\n",
    "    pprint(dataset.summary())\n",
    "    pprint(dataset.abstract())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86364ad2-5b96-4619-b283-82e56e9b6790",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Searching for granules (files) from a given collection (dataset)\n",
    "earthaccess has two different ways of querying for data:\n",
    "1) We can build a query object or \n",
    "2) we can use the top level API. \n",
    "\n",
    "The difference is that the query object is a bit more flexible and we don't retrieve the metadata from CMR until we execute the `.get()` or `.get_all()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6b3bc-7c40-43af-b03d-31616f0fae7d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's use [bboxfinder.com](http://bboxfinder.com) to get the extent of our bounding box and enter it into our bounding box input below. You are also welcome to put in a bounding box you already have available. You can uncomment the Iceland bounding box if you don't want to find your own. Order should be: `min_lon, min_lat, max_lon, max_lat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b74b9-6a08-4938-8ed1-3ef8545eae43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using a specific concept-id, which is unique to the specific product and version\n",
    "\n",
    "# bbox for ~Iceland is -22.1649, 63.3052, -11.9366, 65.5970 \n",
    "granules_query = earthaccess.granule_query().cloud_hosted(True) \\\n",
    "        .concept_id(\"C2021957295-LPCLOUD\") \\\n",
    "        .bounding_box(-22.1649, 63.3052, -11.9366, 65.5970) \\\n",
    "        .temporal(\"2020-01-01\",\"2023-01-01\")\n",
    "granules_query.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ba481-f579-4a5c-a34b-8f19097ae1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "granules_query.hits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38559fcd-4e61-4b7c-bcfb-f8e15ac3b6e6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Earthaccess has many methods we can use for our search. For a complete list of the parameters we can use, go to [https://nsidc.github.io/earthaccess/user-reference/granules/granules-query/](https://nsidc.github.io/earthaccess/user-reference/granules/granules-query/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d48d0-524c-4a08-8845-4c1a8326c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "granule = granules_query.get(1)[0]\n",
    "granule.data_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065b490-7f8d-46be-b35e-3b5e90c734b4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Downloading granules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ef4ad-f8b9-4032-b9e2-0272f096b9a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "**IMPORTANT**: Some datasets will require users to accept an EULA (end user license agreement), it is advisable trying to download a single granule using our browser first and see if we get redirected to a NASA form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe8cd83-b2e1-4c66-8d7c-dcd5271b3916",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "<img src=\"images/EULA.png\" alt=\"NASA end user license agreement)\" class=\"bg-primary mb-1\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e661c-baf1-4d5a-8821-07ef9446829a",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Streaming data with earthaccess\n",
    "If we have enough RAM (memory), we can load our granules from an S3 bucket into memory. Earthaccess works with fsspec (xarray, h5netcdf) at the moment, so this is task is better suited for Level 3 and Level 4 netcdf datasets.\n",
    "\n",
    "We are going to select a few granules for the same day in February for 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f173ef-fb02-4d83-8c35-2fc1b4efd699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iceland_bbox = (-22.1649, 63.3052, -11.9366, 65.5970)\n",
    "# We are going to save our granules for each year on this list\n",
    "granule_list = []\n",
    "\n",
    "for year in range(2018, 2023):\n",
    "    print(f\"Querying {year}\")\n",
    "    granules = earthaccess.search_data(\n",
    "        short_name = \"HLSS30\",\n",
    "        bounding_box = iceland_bbox,\n",
    "        temporal = (f\"{year}-02-17\", f\"{year}-02-18\")\n",
    "    )\n",
    "    granule_list.extend(granules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55302fcc-8396-420f-b3ea-f0587ee5efa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene = granule_list[0]\n",
    "scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92001133-79db-416f-8664-3ba9b8d6e5d0",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Retrieve the data link. \"Direct\" indicates an S3 bucket in the cloud, which you can stream from. \"External\" indicates an HTTPS link which is not cloud-based. Direct access link will allow you to stream the data, but you can also open from HTTPS into memory if it's the right file format (generally formats that Xarray can open). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcca3fa-1319-4a1c-94de-50fb071e06c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Direct access links: \", scene.data_links(access=\"direct\")[0])\n",
    "print(\"External links: \", scene.data_links(access=\"external\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1cbfc-8bdc-4a10-b7de-14185ab86c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import xarray as xr\n",
    "\n",
    "# Read and plot with grid coordinates \n",
    "with rio.open(earthaccess.open(granule_list[1:2])[0]) as src:\n",
    "    fig, ax = plt.subplots(figsize=(9,8))\n",
    "\n",
    "    # To plot\n",
    "    show(src,1)\n",
    "\n",
    "    # To open data into a numpy array\n",
    "    profile = src.profile\n",
    "    arr = src.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55122f3-3989-4589-9b0f-9c378a782e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hls_scene = xr.open_dataset(earthaccess.open(granule_list[1:2])[0], engine='rasterio')\n",
    "hls_scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee74d663-1699-4ea6-88e2-14eeba18f97b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Working with legacy data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f915ebe-ff0f-425d-8594-ba89cd9dbb52",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**MOD07_L2** is an HDF EOS dataset providing atmospheric profiles from the MODerate Resolution Imaging Spectroradiometer instrument on the Terra satelite. You could use this data for providing an atmospheric correction for imagery to get a surface reflectance measurement. \n",
    "\n",
    "It is in a very old HDF data format and although streaming is technically possible, the only libraries that can open HDF EOS expect file paths not remote file systems like the one used by xarray (fsspec). So instead will access the netCDF endpoint via Opendap to download the granules instead of streaming them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5577154-f15c-464c-b68a-57a280d484d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iceland_bbox = (-22.1649, 63.3052, -11.9366, 65.5970)\n",
    "\n",
    "granules = earthaccess.search_data(\n",
    "    short_name = \"MOD07_L2\",\n",
    "    bounding_box = iceland_bbox,\n",
    "    temporal = (f\"2020-01-01\", f\"2020-01-02\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d0974-ecfd-45c5-9c6d-7a1595c167b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Direct access link: \", granules[0].data_links(access=\"direct\"))\n",
    "print(\"External link: \", granules[0].data_links(access=\"external\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfb54d-798a-4bda-88c0-6461bb2c026c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If we try to open these HDF files in Xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a520f7-fb86-40ba-aee6-6a98e7615ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This creates an anticipated error\n",
    "mod07 = xr.open_mfdataset(earthaccess.open(granules[0:3]), engine='rasterio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d208b07-e4cf-4eb7-bb40-7a3da46db3e4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We notice that the access occurs quickly, but xarray is unable to recognize the legacy file format. HDF files are remarkably hard to open. You must download the files and open using pyhdf to open them using code like this: \\\n",
    "`from pyhdf.SD import SD,SDC` \\\n",
    "`mod07_l2 = SD(MODfile, SDC.READ)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec910c79-4b11-43d9-9dbd-7c1d5113ebd3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We do have another option to convert hdf to nc4 files during download so that we can open the files in Xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05800373-0640-44e2-9f7b-09c39ce03059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are going to retrieve the html endpoint link for one granule\n",
    "granules[0]._filter_related_links(\"USE SERVICE API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82568336-a8be-4fa2-9135-c5bebb4148c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "netcdf_list = [g._filter_related_links(\"USE SERVICE API\")[0].replace(\".html\", \".nc4\") for g in granules]\n",
    "netcdf_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116ef2f-e27f-49a2-9e3f-07f70337226f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is going to be slow as we are asking Opendap to format HDF into NetCDF4 so we only processing 3 granules\n",
    "# and Opendap is very prone to failures due concurrent connections, not ideal.\n",
    "# Convert the list from HDF to netcdf4, then download.\n",
    "file_handlers = earthaccess.download(netcdf_list[0:3], local_path=\"test_data\", threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612ab3c-b46c-4da3-9c86-25c6b225b3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the file names\n",
    "path = \"test_data\"\n",
    "dir_list = os.listdir(path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec69a26-cde6-4d9f-8d7a-1092b548910a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cc666-1583-4040-98d6-ae8dd041ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a file into xarray for analysis\n",
    "ds = xr.open_dataset(dir_list[0])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7f36c-b9b9-442e-a9eb-2154cd051534",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now we can remove the test data files and the test_data:\n",
    "1) in file browser on the left, navigate to 'test_data' folder\n",
    "2) delete downloaded files\n",
    "3) navigate one folder up\n",
    "4) delete the 'test_data' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5dacc-e93d-4d13-b760-e0bdfb944bf4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You’ve completed the tutorial. In this tutorial you have gained the skills to:\n",
    "\n",
    "- Transfrom Coordinate Reference Systems, \n",
    "\n",
    "- Open data into Pandas, GeoPandas and Xarray DataFrames/Arrays,\n",
    "\n",
    "- Use Shapely geometries to define an area of interest and subset data,\n",
    "\n",
    "- Learned to use icepyx and earthaccess for streamlining data access, \n",
    "\n",
    "- Search and access optimized and non-optimized cloud data, non-cloud-hosted data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6962a-9f21-418b-adfc-f8af64d127f1",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## References\n",
    "\n",
    "Livingstone, S.J., Li, Y., Rutishauser, A. et al. Subglacial lakes and their changing role in a warming climate. Nat Rev Earth Environ 3, 106–124 (2022). doi:[10.1038/s43017-021-00246-9](https://doi.org/10.1038/s43017-021-00246-9)\n",
    "\n",
    "Smith, B., Fricker, H. A., Holschuh, N., Gardner, A. S., Adusumilli, S., Brunt, K. M., et al. (2019). Land ice height-retrieval algorithm for NASA’s ICESat-2 photon-counting laser altimeter. Remote Sensing of Environment, 233, 111352.  doi:[10.1016/j.rse.2019.111352](https://doi.org/10.1016/j.rse.2019.111352)\n",
    "\n",
    "Smith, B., T. Sutterley, S. Dickinson, B. P. Jelley, D. Felikson, T. A. Neumann, H. A. Fricker, A. Gardner, L. Padman, T. Markus, N. Kurtz, S. Bhardwaj, D. Hancock, and J. Lee. (2022). ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change, Version 2 [Data Set]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. [https://doi.org/10.5067/ATLAS/ATL15.002](https://doi.org/10.5067/ATLAS/ATL15.002). Date Accessed 2023-03-16.\n",
    "\n",
    "Smith, B., T. Sutterley, S. Dickinson, B. P. Jelley, D. Felikson, T. A. Neumann, H. A. Fricker, A. Gardner, L. Padman, T. Markus, N. Kurtz, S. Bhardwaj, D. Hancock, and J. Lee. “ATL15 Data Dictionary (V01).” National Snow and Ice Data Center (NSIDC), 2021-11-29. [https://nsidc.org/data/documentation/atl15-data-dictionary-v01](https://nsidc.org/data/documentation/atl15-data-dictionary-v01). Date Accessed 2023-03-16."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icepyx",
   "language": "python",
   "name": "icepyx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
