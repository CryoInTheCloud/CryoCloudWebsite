{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927cf40c-fe04-439b-a065-07daa155b522",
   "metadata": {},
   "source": [
    "## Using ICESat-2 ATL15, Gridded Arctic Land Ice Height to investigate ice-surface height anomalies\n",
    "\n",
    "#### Written by Wilson Sauthoff (https://wsauthoff.github.io)\n",
    "\n",
    "#### Key learning outcomes:\n",
    "- How to gather data from disparate sources\n",
    "- What is a Coordinate Reference System (CRS) and why it matters.\n",
    "- How to use geometries including Points and Polygons to define an area of interest and subset data. \n",
    "- The basics of how the icepyx library and simplify interacting with ICESat-2 data. \n",
    "- How Xarray can simplify the import of multi-dimensional data.\n",
    "- Open, plot, and explore gridded raster data.\n",
    "\n",
    "#### Practical skills covered to reach key learning outcomes:\n",
    "- Practice importing non-cloud-hosted, cloud-hosted, and available-via-url data to the CryoCloud Jupyter hub\n",
    "- Find CRS information in datasets and reproject another CRS to plot with a dataset with a different CRS.\n",
    "- Create Shapely geometries including Points and Polygons and store in a GeoPandas GeoDataFrames with other data. \n",
    "- Use the icepyx library to simplify the search for ICESat-2 data and the authenication process to access AWS s3 cloud-hosted data.\n",
    "- Import multi-dimensional data using Xarray, access metadata, and use built-in Xarray methods for analysis and data visualization.\n",
    "- Learn the basics of NASA's Ice Cloud, and land Elevation Satellite 2 (ICESat-2) laser altimetry mission.\n",
    "- Learn the details of ICESat-2's high-level data product, ATL15 Gridded Antarctic and Arctic Land Ice Height Change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa3fc55-9a2e-458e-a0a9-3354be7420a5",
   "metadata": {},
   "source": [
    "## Computing environment\n",
    "\n",
    "We'll be using the following open-source Python libraries in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cacc61-87ea-4368-aa57-45f623631d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cells to run command line commands to work off of the icepyx development branch to get the latest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b45e78-1689-471a-aee1-78046fdba29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/icesat2py/icepyx.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f9e3d-a5e3-42c3-a0a9-6175ee77d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd path/to/repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc16f7c-5c65-4117-b1b8-a8a66990af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2db536-38fa-40b2-b01e-6f2f97881789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal libraries\n",
    "import os\n",
    "\n",
    "# import external libraries\n",
    "import geopandas as gpd\n",
    "import h5py\n",
    "import hvplot.xarray\n",
    "import icepyx as ipx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# %matplotlib widgets  # for interactive plots\n",
    "# %matplotlib notebook  # for static plots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import CRS, Transformer\n",
    "import rioxarray\n",
    "import s3fs\n",
    "import xarray as xr\n",
    "\n",
    "# define utility function\n",
    "def ll2ps(lon, lat):\n",
    "    \"\"\"\n",
    "    Transform coordinates from geodetic coordinates (lon, lat)\n",
    "    to Greenland (epsg:3413) coordinates (x, y)\n",
    "    x, y = ll2ps(lon, lat)\n",
    "    Inputs\n",
    "    * lon, lat in decimal degrees (lon: W is negative; lat: S is negative)\n",
    "    Outputs\n",
    "    * x, y in [m]\n",
    "    \"\"\"\n",
    "    crs_ll = CRS(\"EPSG:4326\")\n",
    "    crs_xy = CRS(\"EPSG:3413\")\n",
    "    ll_to_xy = Transformer.from_crs(crs_ll, crs_xy, always_xy = True)\n",
    "    x, y = ll_to_xy.transform(lon, lat)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15816058-825b-46e2-967c-c723ad00c5f0",
   "metadata": {},
   "source": [
    "## In this tutorial, we will focus on Greenland, where active subglacial lakes have been inferred from surface height changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ded922-7e2e-42a4-9435-5399d92fb289",
   "metadata": {},
   "source": [
    "Northern hemisphere subglacial lakes compiled in a global inventory by Livingstone and others (2020):\n",
    "\n",
    "<img src=\"Livingstone_2020_Fig3a.png\" alt=\"2022 inventory of subglacial lakes\" class=\"bg-primary mb-1\" width=\"600px\">\n",
    "\n",
    "We can investigate active subglacial lakes, which drain and fill episodically, using ice surface height anomalies as the overlying ice deforms when active lakes drain and fill. [These videos](https://svs.gsfc.nasa.gov/4913) from NASA's Science Visualization Studio illustrate how we observe active lakes filling and draining through time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488386a-e802-49bc-b93e-bd5d031eec55",
   "metadata": {},
   "source": [
    "## Locating the Greenland active subglacial lakes\n",
    "Consulting the supplementary data of the [Livingstone and others (2020) inventory](https://www.nature.com/articles/s43017-021-00246-9#Sec16), we can construct a [geopandas geodataframe](https://geopandas.org/en/stable/gallery/create_geopandas_from_pandas.html) to investigate Greenland's subglacial lakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36750e1-eb53-426b-bc86-d09d9cf0c3cb",
   "metadata": {},
   "source": [
    "If we are working with a dataset that is not cloud hosted or unavailable via a URL, we can upload that dataset to CryoCloud. \n",
    "\n",
    "First, decide where the uploaded data will live on your CryoCloud hub. It could be a data directory folder that you create in your CryoCloud's base directory or put the data into your working directory (whichever file management technique you prefer). \n",
    "\n",
    "Second, download the paper's supplementary data by [clicking here](https://static-content.springer.com/esm/art%3A10.1038%2Fs43017-021-00246-9/MediaObjects/43017_2021_246_MOESM1_ESM.xlsx). \n",
    "\n",
    "Third, upload the supplementary data spreadsheet into your new data directory folder or your working directory. The supplementary data spreadsheet (.xlsx) is already uploaded to our working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d4ad8-cef2-4d6e-bc88-2ee1d3673c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in spreadsheet using pandas read_excel\n",
    "use_cols = ['Name / Location', 'Lat. oN', 'Lon. oE', 'Lake Type', 'References']\n",
    "import_rows = np.arange(0,65)\n",
    "# You may add a path/to/file if you'd like to try reading spreadsheet from where you saved it in a data directory \n",
    "df = pd.read_excel('43017_2021_246_MOESM1_ESM.xlsx', 'Greenland', usecols=use_cols, skiprows = lambda x: x not in import_rows)\n",
    "\n",
    "# View pandas dataset head (first 5 rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b979f8c-8bb5-446c-8511-283025ff945d",
   "metadata": {},
   "source": [
    "However, since this data is a direct download URL, we can read the data directly into CryoCloud without the tedious download and upload steps, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047bbc78-a049-4055-8002-425d6881b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in spreadsheet using pandas read_excel\n",
    "url = 'https://static-content.springer.com/esm/art%3A10.1038%2Fs43017-021-00246-9/MediaObjects/43017_2021_246_MOESM1_ESM.xlsx'\n",
    "use_cols = ['Name / Location', 'Lat. oN', 'Lon. oE', 'Lake Type', 'References']\n",
    "import_rows = np.arange(0,65)\n",
    "df = pd.read_excel(url, sheet_name='Greenland', usecols=use_cols, skiprows = lambda x: x not in import_rows)\n",
    "\n",
    "# View pandas dataset head (first 5 rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d0285-380e-4e16-9979-73f73170e1cd",
   "metadata": {},
   "source": [
    "This is looking good, but we can make it even better by storing the data in a GeoPandas GeoDataFrame which offers additional functionality beyond pandas of a geometry column of Shapely objects, here Shapely points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7195c0-606b-4467-8b2d-1deb94030b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoPandas GeoDataFrame from Pandas DataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df['Lon. oE'], df['Lat. oN']))\n",
    "\n",
    "# Set the Coordinate Reference System (CRS) of the geodataframe\n",
    "if gdf.crs is None: \n",
    "    # set CRS WGS84 in lon, lat\n",
    "    gdf.set_crs('epsg:4326', inplace=True)\n",
    "    \n",
    "# Display GeoDataFrame\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1751a6-f1fa-4e66-9bc4-c0fed6dcf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look Greenland's active subglacial lake inventory\n",
    "gdf[gdf['Lake Type'] == 'Active']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc8db1-be7f-4933-8b1f-64cbc6af90bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What is a CRS/EPSG?\n",
    "\n",
    "- CRS or epsg are the projection of the data onto a map.\n",
    "- There are numerous formats that are used to document a CRS. Three common formats include: proj.4, EPSG, and Well-known Text (WKT) formats.\n",
    "- Often you have CRS information in one format and you need to transform.\n",
    "- One of the most powerful websites to look up CRS strings is Spatialreference.org. You can use the search on the site to find an EPSG code.\n",
    "- A particular CRS can be referenced by its EPSG code (i.e., epsg:4121). The EPSG is a structured dataset of CRS and Coordinate Transformations. It was originally compiled by the now defunct European Petroleum Survey Group (EPSG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b2914-2a57-40aa-ba26-113e15adf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"Natural Earth” countries dataset, bundled with GeoPandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "Greenland = world[world['name'] == 'Greenland']\n",
    "Greenland = Greenland.to_crs('epsg:3413')\n",
    "    \n",
    "# Create a rough plot to ensure data was read properly\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "Greenland.plot(ax=ax, facecolor='lightgray', edgecolor='gray')\n",
    "gdf[gdf['Lake Type'] == 'Active'].to_crs('epsg:3413').plot(ax=ax, label='active subglacial lake')\n",
    "ax.set_xlabel('northing (m)'); ax.set_ylabel('easting (m)')\n",
    "ax.set_title('Active subglacial lake distribution \\nacross Greenland')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bccee8-6a13-40dd-b59e-ad96602b54d7",
   "metadata": {},
   "source": [
    "### Why we want geometries?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e94b90-a04d-4d8a-9a8b-fb499f9bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoSeries of our GeoDataFrame that converts to Arcic polar stereographic projection \n",
    "# and makes 10-km radius buffered polygon around each lake point\n",
    "gs = gdf.to_crs('3995').buffer(10000)\n",
    "\n",
    "# Create new GeoDataFrame to store the polygons\n",
    "gdf_polys = gpd.GeoDataFrame(gdf, geometry=gs, crs='epsg:3995').to_crs('4623')\n",
    "\n",
    "# Look at active lakes to ensure it worked as expected\n",
    "gdf_polys[gdf_polys['Lake Type'] == 'Active']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96604dff-c7d7-4117-b859-326545f486bc",
   "metadata": {},
   "source": [
    "## What is ICESat-2?\n",
    "\n",
    "ICESat-2 (Ice, Cloud, and land Elevation Satellite 2), part of NASA's Earth Observing System, is a satellite mission for measuring ice sheet elevation and sea ice thickness, as well as land topography, vegetation characteristics, and clouds. It does so using an altimeter or an altitude meter, which is an instrument used to measure the altitude of an object above a fixed level. This is typically achieved by measuring the time it takes for a lidar or radar pulse, released by a satellite-based altimeter, to travel to a surface, reflect, and return to be recorded by an onboard instrument. ICESat-2 uses three pairs of laser pulsers and the detector to count the reflected photons. \n",
    "\n",
    "ICESat-2 laser configuration (from Smith and others, 2019). \n",
    "\n",
    "<img src=\"Smith_2019_fig1.jpg\" alt=\"ICESat-2 laser configuration\" class=\"bg-primary mb-1\" width=\"1000px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c610d-4406-49a5-85be-b26c98a32af0",
   "metadata": {},
   "source": [
    "## What is ATL15?\n",
    "ATL15 is one of the various [ICESat-2 data products](https://icesat-2.gsfc.nasa.gov/science/data-products). ATL15 provides various resolutions (1 km, 10 km, 20 km, and 40 km) height-change maps at 3-month intervals, allowing for visualization of height-change patterns and calculation of integrated regional volume change or integrated active subglacial lake volume change (Smith and others, 2022). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2863c3-3967-47a4-8be6-2eee3d6af0ed",
   "metadata": {},
   "source": [
    "## Streaming cloud-hosted data from NASA Earth Data Cloud\n",
    "We will be working with cloud-hosted data files. This [guide](https://nsidc.org/data/user-resources/help-center/nasa-earthdata-cloud-data-access-guide) explains how to find and access Earthdata cloud-hosted data. And [here](https://nsidc.org/data/earthdata-cloud) is a complete list of earthdata cloud-hosted data products available from NSIDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea8f8c-ce47-4090-919f-d30863309495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to geopackage to subset ICESat-2 data\n",
    "gdf_polys[gdf_polys['Name / Location'] == 'Flade Isblink ice cap'].to_file(os.getcwd() + '/Flade_Isblink_poly.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da0209d-f39a-4566-8966-6798631be98d",
   "metadata": {},
   "source": [
    "## Using icepyx to simplify searching for ICESat-2 data and authenicating with AWS s3 login credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15602f-3fc1-40de-9519-a75ae5b3b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the necessary icepyx parameters\n",
    "short_name = 'ATL15'\n",
    "spatial_extent = 'Flade_Isblink_poly.gpkg' \n",
    "date_range = ['2018-09-15','2023-03-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a352b60-b203-47d6-9785-2cfdbcfdeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Query object\n",
    "region = ipx.Query(short_name, spatial_extent, date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852347c-fafd-4db0-b81e-e70e3e98ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize area of interest\n",
    "region.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bdc52-4de8-44fa-8f8e-fcdd89da01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the available data granuales (files)\n",
    "gran_ids = region.avail_granules()\n",
    "gran_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc30f0-dc2e-4dc4-a13b-305944246691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the granuales IDs and if they are available on the cloud\n",
    "gran_ids = region.avail_granules(ids=True, cloud=True)\n",
    "gran_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e741b-9f35-4598-accf-827e4eb6b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3url = gran_ids[1][0]\n",
    "# s3url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd249ae-dbe8-49c9-90e4-b291613659bf",
   "metadata": {},
   "source": [
    "icepyx doesn't have the s3 urls for the ATL15 data product yet, so we pulled these from NASA Earth Data: https://www.earthdata.nasa.gov/\n",
    "\n",
    "Learn more about finding cloud-hosted data from NASA Earth data cloud [here](https://nsidc.org/data/user-resources/help-center/nasa-earthdata-cloud-data-access-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f78e19-939e-4f7e-b517-d97a90906851",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3url_ATL15 = 's3://nsidc-cumulus-prod-protected/ATLAS/ATL15/002/2019/ATL15_GL_0314_01km_002_01.nc'\n",
    "# learn more about the ICESat-2 ATL15 Gridded Antarctic and Arctic Land Ice Height Change data product dataset here: \n",
    "# https://doi.org/10.5067/ATLAS/ATL15.002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba938e7-0ac0-4c6f-99f1-854a57ac2779",
   "metadata": {},
   "source": [
    "The next step requires a NASA Earth Data user account. You can register for a free account [here](https://www.earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/earthdata-login). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab93236-ea62-4f53-a5ab-077a160feaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthdata_uid = 'wsauthoff'  # Replace with your NASA Earth Data user ID\n",
    "earthdata_email = 'wilson.sauthoff@gmail.com' # Replace with your NASA Earth Data email\n",
    "\n",
    "# Authenicate using your NASA Earth Data login credentials; enter your password when prompted\n",
    "region.earthdata_login(earthdata_uid, earthdata_email, s3token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e81f6-9092-4523-8565-34a75d8631f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = region._s3login_credentials\n",
    "\n",
    "s3 = s3fs.S3FileSystem(key=credentials['accessKeyId'],\n",
    "                       secret=credentials['secretAccessKey'],\n",
    "                       token=credentials['sessionToken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377ae88-8552-4ee8-8595-d15b786b003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open s3url data file and store in Xarray Dataset\n",
    "# This cell takes 10s of secs to load\n",
    "with s3.open(s3url_ATL15,'rb') as f:\n",
    "    # ATL15_dh = rioxarray.open_rasterio(f, group='delta_h').load()  # FIXME: preferred, but giving error\n",
    "    ATL15_dh = xr.open_dataset(f, group='delta_h').load()\n",
    "\n",
    "# View Xarray Dataset\n",
    "ATL15_dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e5f8a-a7fb-4f76-b70d-cd9c55c94f08",
   "metadata": {},
   "source": [
    "We can acquaint ourselves with this dataset in a few ways: \n",
    "- The data product's [overview page](https://doi.org/10.5067/ATLAS/ATL15.002) (Smith and others, 2022) to get the very basics such as geographic coverage, CRS, and what the data product tells us (quarterly height changes).\n",
    "- The Xarray Dataset read-in metadata: clicking on the written document icon of each data variable will expand metadata including a data variable's dimensions, datatype, etc. \n",
    "- The data product's [data dictionary](https://nsidc.org/data/documentation/atl15-data-dictionary-v01) (Smith and others, 2021) to do a deep dive on what individual variables tell us. \n",
    "\n",
    "We'll be plotting the delta_h data variable in this tutorial, here's what we can learn about from our these sources:\n",
    "- Overview page: this is likely the 'quarterly height changes' described, but let's dive deeper to be sure\n",
    "- Xarray Dataset imbedded metadata tells us a couple things: height change  at 1 km (the resolution selected earlier) and height change relative to the datum (Jan 1, 2020) surface\n",
    "\n",
    "Ok, since the data is relative to a datum, we have two options: \n",
    "1) Difference individual time slices to subtract out the datum, like so: \n",
    "\n",
    "    (time_0 - datum) - (time_1 - datum) = time_0 - datum - time_1 + datum = time_0 - time_1\n",
    "\n",
    "2) Subtract out the datum directly. The datum is the complementary dataset ATL14: a high-resolution (100 m) digital elevation model (DEM) that provides spatially continuous gridded data of ice sheet surface height. [NEED TO VERIFY THIS; DATA DICTIONARY IS NOT EXPLICIT]\n",
    "\n",
    "In this tutorial we'll use the first method. We'll use some explanatory data analysis to illustrate this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9c230-1b92-4468-8231-5f90a9f119ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a simple plot of the first minus the zeroth time slices\n",
    "fig, ax = plt.subplots()\n",
    "dhdt = ATL15_dh['delta_h'][1,:,:] - ATL15_dh['delta_h'][0,:,:]\n",
    "cb = ax.imshow(dhdt, origin='lower', norm=mpl.colors.CenteredNorm(), cmap='coolwarm_r', \n",
    "               extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "ax.set_xlabel('northing (m)'); ax.set_ylabel('easting (m)')\n",
    "plt.colorbar(cb, label='height change [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c743f6-7df7-4bd4-a407-547179f985d3",
   "metadata": {},
   "source": [
    "Hmmm...doesn't look like much change over this quarter. Why? Check out the bounds of the colorbar, we've got some pretty extreme values (colorbar is defaulting to ±10 m!) that appear to be along the margin. It's making more sense now. We can change the bounds of the colorbar to plot see more of the smaller scale change in the continental interior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b2eae-00fc-4a4b-8343-28b38688433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate some basic stats to determine appropriate coloarbar bounds\n",
    "print(dhdt.min().values)\n",
    "print(dhdt.max().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb944c-b268-44cb-a4f3-690e8a7ae8ef",
   "metadata": {},
   "source": [
    "We can use a TwoSlopeNorm to achieve different mapping on either side the center at zero: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7223f-02fa-4570-9704-9c631b3b996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make that same plot with more representative colorbar bounds\n",
    "fig, ax = plt.subplots()\n",
    "dhdt = ATL15_dh['delta_h'][1,:,:] - ATL15_dh['delta_h'][0,:,:]\n",
    "divnorm = colors.TwoSlopeNorm(vmin=dhdt.min(), vcenter=0, vmax=dhdt.max())\n",
    "cb = ax.imshow(dhdt, origin='lower', norm=divnorm, cmap='coolwarm_r', \n",
    "               extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "ax.set_xlabel('northing (m)'); ax.set_ylabel('easting (m)')\n",
    "plt.colorbar(cb, label='height change [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b76be-6df8-4cb1-b475-d962a7d7eb06",
   "metadata": {},
   "source": [
    "Now the colorbar bounds are more representative, but we still have the issue that extreme values along the margin are swapping any signals we might see in the continental interior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82678fa1-fd9a-469f-9eac-2254624f5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the Xarray.DataArray quantile method to find the 1% and 99% quantiles (Q1 and Q3) of the data\n",
    "dhdt.quantile([0.01,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff22ef-89ee-4f85-9b14-e46dfe528978",
   "metadata": {},
   "source": [
    "We can use these quantiles as the colorbar bounds so that we see the data variability by plotting the most extreme values at the maxed out value of the colorbar. We'll adjust the caps of the colorbar to express that there are data values beyond the bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320be7d-73dd-45ec-8a7f-7efdb9b629d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the same plot but using the quantiles as the colorbar bounds\n",
    "fig, ax = plt.subplots()\n",
    "dhdt = ATL15_dh['delta_h'][1,:,:] - ATL15_dh['delta_h'][0,:,:]\n",
    "divnorm = colors.TwoSlopeNorm(vmin=dhdt.quantile(0.01), vcenter=0, vmax=dhdt.quantile(0.99))\n",
    "cb = ax.imshow(dhdt, origin='lower', norm=divnorm, cmap='coolwarm_r', \n",
    "               extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "ax.set_xlabel('northing (m)'); ax.set_ylabel('easting (m)')\n",
    "plt.colorbar(cb, extend='both', label='height change [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dab15-820d-45d9-bf17-72a8702af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the same plot but for all the available time slices and let's turn it in a function so that we can reuse this code for a smaller subset of data\n",
    "# create empty lists to store data\n",
    "def plot_icesat2_atl15(xmin, xmax, ymin, ymax, dataset):\n",
    "    # subset data using bounding box in epsg:3134 x,y\n",
    "    mask_x = (dataset.x >= xmin) & (dataset.x <= xmax)\n",
    "    mask_y = (dataset.y >= ymin) & (dataset.y <= ymax)\n",
    "    ds_sub = dataset.where(mask_x & mask_y, drop=True)\n",
    "    \n",
    "    # Create empty lists to store data\n",
    "    vmins_maxs = []\n",
    "\n",
    "    # Find the min's and max's of each inter time slice comparison and store into lists\n",
    "    for idx in range(len(ds_sub['time'].values)-1): \n",
    "        dhdt = ds_sub['delta_h'][idx+1,:,:] - ds_sub['delta_h'][idx,:,:]\n",
    "        vmin=dhdt.quantile(0.01)\n",
    "        vmins_maxs += [vmin]\n",
    "        vmax=dhdt.quantile(0.99)\n",
    "        vmins_maxs += [vmax]\n",
    "        if (min(vmins_maxs)<0) & (max(vmins_maxs)>0):\n",
    "            vcenter = 0\n",
    "        else: \n",
    "            vcenter = max(vmins_maxs) - min(vmins_maxs)\n",
    "        divnorm = colors.TwoSlopeNorm(vmin=min(vmins_maxs), vcenter=vcenter, vmax=max(vmins_maxs))\n",
    "\n",
    "    # create fig, ax\n",
    "    fig, axs = plt.subplots(2,7, sharex=True, sharey=True, figsize=(10,4))\n",
    "\n",
    "    idx = 0\n",
    "    for ax in axs.ravel():   \n",
    "        ax.set_aspect('equal')\n",
    "        dhdt = ds_sub['delta_h'][idx+1,:,:] - ds_sub['delta_h'][idx,:,:]\n",
    "        cb = ax.imshow(dhdt, origin='lower', norm=divnorm, cmap='coolwarm_r', \n",
    "                       extent=[Greenland.bounds.minx.values[0], Greenland.bounds.maxx.values[0], Greenland.bounds.miny.values[0], Greenland.bounds.maxy.values[0]])\n",
    "        # Change polar stereographic m to km\n",
    "        km_scale = 1e3\n",
    "        ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax.xaxis.set_major_formatter(ticks_x)\n",
    "        ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/km_scale))\n",
    "        ax.yaxis.set_major_formatter(ticks_y)\n",
    "        # Create common axes labels\n",
    "        fig.supxlabel('easting (m)'); fig.supylabel('northing (m)')\n",
    "        # Increment the idx\n",
    "        idx = idx + 1\n",
    "     \n",
    "    fig.colorbar(cb, ax=axs.ravel().tolist())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6cefc-7794-45e3-a5d6-3900d7204d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom into an individual active lake to see more detail\n",
    "gdf_polys[gdf_polys['Lake Type'] == 'Active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54bca1-def0-46cd-b2c0-d08a1e79611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can call the bounds of the geometry of the Shapely Polygon we created earlier\n",
    "gdf_sub = gdf_polys[gdf_polys['Name / Location'] == 'Inuppaat Quuat']\n",
    "gdf_sub.geometry.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e949e39-f84f-42ff-8e7f-7b45270d9a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_min = gdf_sub.geometry.bounds.minx\n",
    "lon_max = gdf_sub.geometry.bounds.maxx\n",
    "lat_min = gdf_sub.geometry.bounds.miny\n",
    "lat_max = gdf_sub.geometry.bounds.maxy\n",
    "xmin, ymin = ll2ps(lon_min, lat_min)\n",
    "xmax, ymax = ll2ps(lon_max, lat_max)\n",
    "\n",
    "plot_icesat2_atl15(xmin, xmax, ymin, ymax, ATL15_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8ac89-9c5d-4088-b7d1-6abff3147f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Xarray to export the time series into a gif\n",
    "# PLACEHOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a59c54-792e-4a6b-8cbb-590d555405a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore plotting the data interactively using Xarray and Holoviews\n",
    "hvplot.extension('matplotlib')\n",
    "divnorm = colors.TwoSlopeNorm(vmin=-5, vcenter=0, vmax=5)\n",
    "ATL15_dh['delta_h'].hvplot(groupby='time', cmap='coolwarm_r', norm=divnorm, invert=True, \n",
    "                           width=(ATL15_dh['x'].max()-ATL15_dh['x'].min())/3e3, height=(ATL15_dh['y'].max()-ATL15_dh['y'].min())/3e3, \n",
    "                           widget_type='scrubber', widget_location='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4a56e-23dd-492d-b433-14412e22590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up environment by deleting intermediary files\n",
    "os.remove('Flade_Isblink_poly.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373dba30-f76d-4eea-8a04-8105531fab0a",
   "metadata": {},
   "source": [
    "## Streaming cloud-hosted data via EarthAccess (PLACEHOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723648c-2f77-4746-9b2c-4fb207ab96eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac6962a-9f21-418b-adfc-f8af64d127f1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Livingstone, S.J., Li, Y., Rutishauser, A. et al. Subglacial lakes and their changing role in a warming climate. Nat Rev Earth Environ 3, 106–124 (2022). https://doi.org/10.1038/s43017-021-00246-9\n",
    "\n",
    "Smith, B., Fricker, H. A., Holschuh, N., Gardner, A. S., Adusumilli, S., Brunt, K. M., et al. (2019). Land ice height-retrieval algorithm for NASA’s ICESat-2 photon-counting laser altimeter. Remote Sensing of Environment, 233, 111352. https://doi.org/10.1016/j.rse.2019.111352\n",
    "\n",
    "Smith, B., T. Sutterley, S. Dickinson, B. P. Jelley, D. Felikson, T. A. Neumann, H. A. Fricker, A. Gardner, L. Padman, T. Markus, N. Kurtz, S. Bhardwaj, D. Hancock, and J. Lee. (2022). ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change, Version 2 [Data Set]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/ATLAS/ATL15.002. Date Accessed 2023-03-16.\n",
    "\n",
    "Smith, B., T. Sutterley, S. Dickinson, B. P. Jelley, D. Felikson, T. A. Neumann, H. A. Fricker, A. Gardner, L. Padman, T. Markus, N. Kurtz, S. Bhardwaj, D. Hancock, and J. Lee. “ATL15 Data Dictionary (V01).” National Snow and Ice Data Center (NSIDC), 2021-11-29. https://nsidc.org/data/documentation/atl15-data-dictionary-v01. Date Accessed 2023-03-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c815c70-23b8-40e6-a0f5-d7a4db455297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
